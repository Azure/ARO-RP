# Azure DevOps Pipeline running CI and E2E

trigger:
  branches:
    include:
      - master
  paths:
    exclude:
      - docs/*
  tags:
    include:
      - v2*

variables:
  - template: vars.yml
  - name: REGISTRY
    value: registry.access.redhat.com
  - name: LOCAL_ARO_RP_IMAGE
    value: "localhost/aro"              # Local image for ARO RP
  - name: LOCAL_ARO_AZEXT_IMAGE
    value: "arosvcdev.azurecr.io/azext-aro"
  - name: LOCAL_VPN_IMAGE
    value: "arosvcdev.azurecr.io/vpn"
  - name: LOCAL_E2E_IMAGE
    value: "localhost/e2e"              # Local image for E2E
  - name: TAG
    value: $(Build.BuildId)
  - name: VERSION
    value: $(Build.BuildId)

stages:
  - stage: Containerized
    jobs:
      - job: Build_Test_And_Push_Az_ARO_Extension
        pool:
          name: 1es-aro-ci-pool
        steps:
          - template: ./templates/template-checkout.yml

          # Build and test the Az ARO Extension
          - script: |
              set -xe
              DOCKER_BUILD_CI_ARGS="--load" make ci-azext-aro VERSION=$(VERSION)
            displayName: ðŸ›  Build & Test Az ARO Extension

          # Push the image to ACR
          - template: ./templates/template-acr-push.yml
            parameters:
              acrFQDN: 'arosvcdev.azurecr.io'
              repository: 'azext-aro'
              pushLatest: true

      - job: Build_And_Test_RP_And_Portal
        pool:
          name: 1es-aro-ci-pool
        steps:
          - template: ./templates/template-checkout.yml

          # Build and test RP and Portal
          - script: |
              set -xe
              DOCKER_BUILD_CI_ARGS="--load" make ci-rp VERSION=$(VERSION)
            displayName: ðŸ›  Build & Test RP and Portal

          # Publish test results
          - task: PublishTestResults@2
            displayName: ðŸ“Š Publish tests results
            inputs:
              testResultsFiles: $(System.DefaultWorkingDirectory)/report.xml
            condition: succeededOrFailed()

          # Publish code coverage results
          - task: PublishCodeCoverageResults@2
            displayName: ðŸ“ˆ Publish code coverage
            inputs:
              codeCoverageTool: Cobertura
              summaryFileLocation: $(System.DefaultWorkingDirectory)/coverage.xml
              failIfCoverageEmpty: false
            condition: succeededOrFailed()

          # Push the RP image to ACR
          - template: ./templates/template-acr-push.yml
            parameters:
              acrFQDN: 'arosvcdev.azurecr.io'
              repository: 'aro'
              pushLatest: true

  - stage: E2E  # E2E Stage using Docker Compose
    dependsOn: Containerized
    jobs:
      - job: Run_E2E_Tests
        pool:
          name: 1es-aro-ci-pool
        steps:
          # Step 1: Checkout the code
          - template: ./templates/template-checkout.yml

          # Step 2: Install Docker, Docker Compose, and dependencies
          - bash: |
              export CI=true
              . ./hack/e2e/utils.sh
              install_docker_dependencies
            displayName: Install Docker and Docker Compose

          # Step 3: AZ CLI Login
          - template: ./templates/template-az-cli-login.yml
            parameters:
              azureDevOpsJSONSPN: $(aro-v4-e2e-devops-spn)

          # Step 4: Get Kubeconfig for AKS Cluster with corrected Key Vault configuration
          - bash: |
              az account set -s $AZURE_SUBSCRIPTION_ID
              SECRET_SA_ACCOUNT_NAME=$(SECRET_SA_ACCOUNT_NAME) make secrets
              . secrets/env
              export CI=true
              export KEYVAULT_PREFIX="e2e-classic-eastus-cls"
              KEYVAULT_URL="https://${KEYVAULT_PREFIX}.vault.azure.net"

              # Retrieve the kubeconfig
              hack/get-admin-aks-kubeconfig.sh > aks.kubeconfig

              if [ -f aks.kubeconfig ]; then
                echo "Kubeconfig retrieved successfully."
                echo "KUBECONFIG=$(pwd)/aks.kubeconfig" >> .env
                cat aks.kubeconfig
              else
                echo "Failed to retrieve Kubeconfig."
                exit 1
              fi
            displayName: Get Kubeconfig for AKS Cluster

          # Step 5: Deploy Hive Operator with fix for unbound PULL_SECRET
          - bash: |
              az account set -s $AZURE_SUBSCRIPTION_ID
              SECRET_SA_ACCOUNT_NAME=$(SECRET_SA_ACCOUNT_NAME) make secrets
              . secrets/env
              export CI=true
              docker compose --env-file .env -f docker-compose.yml up -d vpn
              while [ "$(docker inspect --format '{{.State.Health.Status}}' vpn)" != "healthy" ]; do
                echo "Waiting for VPN to be healthy..."
                sleep 10
              done
              docker ps
              export KUBECONFIG=$(pwd)/aks.kubeconfig
              kubectl get nodes
              ./hack/hive/hive-dev-install.sh
            displayName: Deploy Hive Operator

          # Step 6: Start Services using Docker Compose
          - bash: |
              az account set -s $AZURE_SUBSCRIPTION_ID
              SECRET_SA_ACCOUNT_NAME=$(SECRET_SA_ACCOUNT_NAME) make secrets
              . secrets/env
              export CI=true
              export ARO_SELENIUM_HOSTNAME="localhost"
              export OS_CLUSTER_VERSION=4.13.40
              export RP_IMAGE_ACR=arosvcdev.azurecr.io
              export LOCAL_E2E_IMAGE=${LOCAL_E2E_IMAGE}    # Consistent with LOCAL_E2E_IMAGE
              export E2E_LABEL='!smoke&&!regressiontest'
              . ./hack/e2e/run-rp-and-e2e.sh
              deploy_e2e_db
              register_sub
              env | sort
              docker ps
              docker compose up run-e2e
              docker ps
            displayName: Start Services using Docker Compose

          # Step 8: Log the output from the services in case of failure
          - bash: |
              docker compose logs portal
              docker compose logs vpn
              docker compose logs selenium
              docker compose logs rp
              docker compose logs run-e2e
            displayName: Log Service Output
            condition: always()

          # must-gather collection must be run inside the container so it can access the VPN
          - bash: |
              export CI=true
              hack/get-admin-aks-kubeconfig.sh > aks.kubeconfig
            displayName: Get admin kubeconfig for must-gather
            condition: failed()
          - bash: |
              export CI=true
              export KUBECONFIG=aks.kubeconfig
              wget -nv https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/$(OpenShiftCLIVersion)/openshift-client-linux-$(OpenShiftCLIVersion).tar.gz
              tar xf openshift-client-linux-$(OpenShiftCLIVersion).tar.gz
              ./oc adm must-gather --image quay.io/cmarches/aro-must-gather:20231030.00
              tar cf must-gather.tar.gz must-gather.local.*
            displayName: Collect must-gather
            condition: failed()
          # Publish the must-gather result to the pipeline
          - publish: must-gather.tar.gz
            artifact: must-gather
            displayName: Append must-gather to Pipeline
            condition: failed()

          # Step 9: Clean up Docker Compose services
          - bash: |
              docker compose down
              rm -f aks.kubeconfig
            displayName: Cleanup Docker Compose Services and Kubeconfig
            condition: always()

          # Step 10: Clean Up Database
          - bash: |
              az cosmosdb sql database delete --name "$DATABASE_NAME" --yes --account-name "$DATABASE_ACCOUNT_NAME" --resource-group "$RESOURCEGROUP"
            displayName: Clean Up Database
            condition: always()

          # Step 11: Cleanup Hive Operator
          - bash: |
              echo "Cleaning up Hive Operator..."
              kubectl delete namespace hive || echo "Namespace already deleted or does not exist."
            displayName: Cleanup Hive Operator
            condition: always()

          # Step 12: AZ CLI Logout
          - template: ./templates/template-az-cli-logout.yml
