# Azure DevOps Pipeline running CI and E2E

trigger:
  branches:
    include:
      - master
  paths:
    exclude:
      - docs/*
      - "**/*.md"
      - LICENSE
      - LICENSE.txt
      - .gitignore
      - OWNERS
      - .github/CODEOWNERS
      - .github/PULL_REQUEST_TEMPLATE.md
      - .github/dependabot.yml
      - .github/workflows/check-coverage.yml
      - .github/workflows/codeql-analysis.yml
      - .github/workflows/maintenance.yml
      - .github/workflows/npm-audit.yml
      - .github/workflows/prune-tags.yml
      - .github/workflows/release-note.yml
      - .github/workflows/yamllint.yml
  tags:
    include:
      - v2*

# PR triggers defined in YAML to override ADO UI and respect path exclusions
pr:
  branches:
    include:
      - master
  paths:
    exclude:
      - docs/*
      - "**/*.md"
      - LICENSE
      - LICENSE.txt
      - .gitignore
      - OWNERS
      - .github/CODEOWNERS
      - .github/PULL_REQUEST_TEMPLATE.md
      - .github/dependabot.yml
      - .github/workflows/check-coverage.yml
      - .github/workflows/codeql-analysis.yml
      - .github/workflows/maintenance.yml
      - .github/workflows/npm-audit.yml
      - .github/workflows/prune-tags.yml
      - .github/workflows/release-note.yml
      - .github/workflows/yamllint.yml

variables:
  - template: vars.yml
  - name: CI
    value: true
  - name: RP_IMAGE_ACR
    value: arointsvc
  - name: REGISTRY
    value: arointsvc.azurecr.io
  - name: BUILDER_REGISTRY
    value: arointsvc.azurecr.io
  - name: LOCAL_ARO_RP_IMAGE
    value: arosvcdev.azurecr.io/aro
  - name: LOCAL_ARO_AZEXT_IMAGE
    value: arosvcdev.azurecr.io/azext-aro
  - name: LOCAL_VPN_IMAGE
    value: arosvcdev.azurecr.io/vpn
  - name: LOCAL_E2E_IMAGE
    value: arosvcdev.azurecr.io/e2e
  - name: VERSION
    value: $(TAG)
  - name: TAG
    value: default-tag # Default fallback TAG
  - name: ARO_IMAGE
    value: arosvcdev.azurecr.io/aro:$(TAG)
  - name: ARO_SELENIUM_HOSTNAME
    value: localhost
  - name: E2E_LABEL
    value: "!smoke&&!regressiontest"

stages:
  - stage: Set_Tag_Stage
    displayName: "Set Dynamic TAG Variable"
    jobs:
      - job: Set_Tag_Variable
        displayName: "Set TAG Variable"
        pool:
          name: 1es-aro-ci-pool
        steps:
          - checkout: self

          - script: |
              echo "Setting TAG variable..."
              if [ -n "$(System.PullRequest.PullRequestId)" ] && [ -n "$(System.PullRequest.SourceCommitId)" ]; then
                TAG="pr-$(System.PullRequest.PullRequestId)-$(System.PullRequest.SourceCommitId)"
              elif [ -n "$(Build.SourceVersion)" ]; then
                TAG="master-$(Build.SourceVersion)"
              else
                TAG="default-tag"
              fi
              echo "##vso[task.setvariable variable=TAG;isOutput=true]$TAG"
              echo "TAG set to: $TAG"
            name: SetTag
            displayName: "Set TAG Variable"

  - stage: Containerized_CI
    displayName: "Containerized CI"
    dependsOn: Set_Tag_Stage
    variables:
      TAG: $[ stageDependencies.Set_Tag_Stage.Set_Tag_Variable.outputs['SetTag.TAG'] ]

    jobs:
      - job: Build_Test_And_Push_Az_ARO_Extension
        displayName: "Build and Push Az ARO Extension"
        pool:
          name: 1es-aro-ci-pool
        steps:
          - checkout: self
          - task: Docker@2
            inputs:
              command: "login"
              containerRegistry: arointsvc #service connection name

          # Clean up stale Docker state from previous runs on this node.
          # Only removes stopped containers, dangling images, and old build cache.
          - template: ./templates/template-docker-cleanup.yml
            parameters:
              reason: "pre-build"

          # Build and Test Az ARO Extension
          - script: |
              set -xe
              echo "Building with TAG: $(TAG)"
              DOCKER_BUILD_CI_ARGS="--load" make ci-azext-aro VERSION=$(TAG)
            displayName: "üõ† Build & Test Az ARO Extension"

          # Push the image to ACR
          - template: ./templates/template-acr-push.yml
            parameters:
              acrFQDN: "arosvcdev.azurecr.io"
              repository: "azext-aro"
              tag: $(TAG)
              pushLatest: true

          # Post-build cleanup: remove stopped containers and dangling images
          # so they don't accumulate across runs on persistent CI nodes.
          - template: ./templates/template-docker-cleanup.yml
            parameters:
              reason: "post-build-azext"

      - job: Build_And_Test_RP_And_Portal
        pool:
          name: 1es-aro-ci-pool
        steps:
          - template: ./templates/template-checkout.yml
          - task: Docker@2
            inputs:
              command: "login"
              containerRegistry: arointsvc #service connection name

          # Clean up stale Docker state from previous runs on this node.
          # Only removes stopped containers, dangling images, and old build cache.
          - template: ./templates/template-docker-cleanup.yml
            parameters:
              reason: "pre-build"

          # Build and test RP and Portal
          # Note: ADO masks $(REGISTRY) and $(BUILDER_REGISTRY) in all log
          # output because they match pipeline variable values. This is
          # cosmetic ‚Äî the values ARE passed correctly (build succeeds).
          # The InvalidDefaultArgInFrom warnings are a BuildKit static lint
          # check on ARGs with no default; harmless since --build-arg is set.
          - script: |
              set -xe
              DOCKER_BUILD_CI_ARGS="--load" \
              REGISTRY="$(REGISTRY)" \
              BUILDER_REGISTRY="$(BUILDER_REGISTRY)" \
              VERSION="$(TAG)" \
              LOCAL_ARO_RP_IMAGE="$(LOCAL_ARO_RP_IMAGE)" \
              make ci-rp
            displayName: üõ† Build & Test RP and Portal

          # Publish test results
          - task: PublishTestResults@2
            displayName: üìä Publish tests results
            inputs:
              testResultsFiles: $(System.DefaultWorkingDirectory)/report.xml
            condition: succeededOrFailed()

          # Publish code coverage results
          - task: PublishCodeCoverageResults@2
            displayName: üìà Publish code coverage
            inputs:
              codeCoverageTool: Cobertura
              summaryFileLocation: $(System.DefaultWorkingDirectory)/coverage.xml
              failIfCoverageEmpty: false
            condition: succeededOrFailed()

          # Push the RP image to ACR
          - template: ./templates/template-acr-push.yml
            parameters:
              acrFQDN: "arosvcdev.azurecr.io"
              repository: "aro"
              tag: $(TAG)
              pushLatest: true

          # Post-build cleanup: remove stopped containers and dangling images
          # so they don't accumulate across runs on persistent CI nodes.
          - template: ./templates/template-docker-cleanup.yml
            parameters:
              reason: "post-build-rp"

      - job: Build_And_Push_E2E_Image
        pool:
          name: 1es-aro-ci-pool
        steps:
          - template: ./templates/template-checkout.yml
          - task: Docker@2
            inputs:
              command: "login"
              containerRegistry: arointsvc #service connection name

          # Clean up stale Docker state from previous runs on this node.
          # Only removes stopped containers, dangling images, and old build cache.
          - template: ./templates/template-docker-cleanup.yml
            parameters:
              reason: "pre-build"

          # Build the E2E image
          - script: |
              set -xe
              DOCKER_BUILD_CI_ARGS="--load" make aro-e2e VERSION=$(TAG)
            displayName: üõ† Build the E2E image

          # Push the E2E image to ACR
          - template: ./templates/template-acr-push.yml
            parameters:
              acrFQDN: "arosvcdev.azurecr.io"
              repository: "e2e"
              tag: $(TAG)
              pushLatest: true

          # Post-build cleanup: remove stopped containers and dangling images
          # so they don't accumulate across runs on persistent CI nodes.
          - template: ./templates/template-docker-cleanup.yml
            parameters:
              reason: "post-build-e2e"

  - stage: E2E # E2E Stage using Docker Compose
    dependsOn: [Set_Tag_Stage, Containerized_CI]
    variables:
      TAG: $[ stageDependencies.Set_Tag_Stage.Set_Tag_Variable.outputs['SetTag.TAG'] ]
    jobs:
      - job: Run_E2E_Tests_For_CSP
        timeoutInMinutes: 0
        pool:
          name: 1es-aro-ci-pool
        steps:
          # Checkout the code
          - template: ./templates/template-checkout.yml

          # Install Docker, Docker Compose, and dependencies
          - bash: |
              . ./hack/e2e/utils.sh
              install_docker_dependencies
            displayName: Install Docker and Docker Compose
          - task: Docker@2
            inputs:
              command: "login"
              containerRegistry: arointsvc #service connection name

          # Clean up stale Docker state from previous runs on this node.
          # Only removes stopped containers, dangling images, and old build cache.
          - template: ./templates/template-docker-cleanup.yml
            parameters:
              reason: "pre-e2e"

          # AZ CLI Login
          - template: ./templates/template-az-cli-login.yml
            parameters:
              azureDevOpsJSONSPN: $(aro-v4-e2e-devops-spn)

          - bash: |
              az account set -s $AZURE_SUBSCRIPTION_ID
              SECRET_SA_ACCOUNT_NAME=$(SECRET_SA_ACCOUNT_NAME) make secrets
            displayName: Fetch secrets

          - bash: |
              echo "##vso[task.setvariable variable=CI]true"
            displayName: Set CI=true

          # Override the E2E label for IndividualCI/BatchedCI (i.e. not manually
          # ran/PR jobs) to run all non-smoke tasks (default is !smoke&&!regressiontest)
          - bash: |
              echo "##vso[task.setvariable variable=E2E_LABEL]!smoke"
            displayName: Enable regression tests in CI
            condition: in(variables['Build.Reason'], 'IndividualCI', 'BatchedCI')

          # Get Kubeconfig for AKS Cluster with corrected Key Vault configuration
          - bash: |
              . secrets/env

              # Retrieve the kubeconfig
              hack/get-admin-aks-kubeconfig.sh > aks.kubeconfig

              if [ -f aks.kubeconfig ]; then
                echo "Kubeconfig retrieved successfully."
              else
                echo "Failed to retrieve Kubeconfig."
                exit 1
              fi
            displayName: Get Kubeconfig for AKS Cluster

          # Run the E2E test suite
          - bash: |
              . ./hack/e2e/run-rp-and-e2e.sh
              echo "##vso[task.setvariable variable=CLUSTER]${CLUSTER}"
              echo "##vso[task.setvariable variable=DATABASE_NAME]${DATABASE_NAME}"
            displayName: Set CLUSTER and DATABASE_NAME
          - bash: |
              . ./hack/e2e/run-rp-and-e2e.sh
              az acr login --name arosvcdev
              az acr login --name arointsvc

              deploy_e2e_db
              register_sub
              docker compose build vpn
              docker compose up --quiet-pull --exit-code-from e2e e2e
              # Check if the E2E tests failed
              E2E_EXIT_CODE=$?
              if [ $E2E_EXIT_CODE -ne 0 ]; then
                echo "##vso[task.logissue type=error] E2E tests failed. Check the logs for more details."
                exit $E2E_EXIT_CODE
              else
                echo "E2E tests passed."
              fi
            displayName: ‚öôÔ∏è  Run E2E Test Suite for CSP

          # Log the output from the services in case of failure
          - bash: |
              docker compose logs vpn
              docker compose logs selenium
              docker compose logs rp
              docker compose logs portal
              docker compose logs e2e
            displayName: Log Service Output
            condition: always()

          # Collect must-gather logs
          - bash: |
              # The script calls ./db <resource-id> which connects to the CosmosDB to fetch cluster details.
              # Since must-gather runs independently, we rebuild the binary on-demand.
              . secrets/env
              go build ./hack/db
              ./hack/get-admin-kubeconfig.sh /subscriptions/$AZURE_SUBSCRIPTION_ID/resourceGroups/$CLUSTER/providers/Microsoft.RedHatOpenShift/openShiftClusters/$CLUSTER > admin.kubeconfig
              export KUBECONFIG=$(pwd)/admin.kubeconfig
              wget -nv https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/$(OpenShiftCLIVersion)/openshift-client-linux-$(OpenShiftCLIVersion).tar.gz
              tar xf openshift-client-linux-$(OpenShiftCLIVersion).tar.gz
              ./oc adm must-gather --image arointsvc.azurecr.io/openshift4/ose-must-gather:latest
              tar cf must-gather.tar.gz must-gather.local.*
            displayName: Collect must-gather
            condition: Failed()
          # Publish the must-gather result to the pipeline
          - publish: must-gather.tar.gz
            artifact: must-gather
            displayName: Append must-gather to Pipeline
            condition: Failed()

          # Clean up Docker Compose services with diagnostics
          - bash: |
              set -x
              echo "=== Docker state before compose down ==="
              echo "--- Running containers ---"
              docker ps --format "table {{.ID}}\t{{.Image}}\t{{.Status}}\t{{.Names}}" || true
              echo "--- All containers (including stopped) ---"
              docker ps -a --format "table {{.ID}}\t{{.Image}}\t{{.Status}}\t{{.Names}}" || true
              echo "--- Docker images on this node ---"
              docker images --format "table {{.Repository}}\t{{.Tag}}\t{{.ID}}\t{{.CreatedSince}}\t{{.Size}}" || true
              echo "--- Docker disk usage ---"
              docker system df || true
              echo ""
              echo "=== Stopping compose services ==="
              docker compose down --remove-orphans || true
              rm -f aks.kubeconfig
              echo ""
              echo "=== Docker state after compose down ==="
              docker ps -a --format "table {{.ID}}\t{{.Image}}\t{{.Status}}\t{{.Names}}" || true
            displayName: Cleanup Docker Compose Services and Kubeconfig (with diagnostics)
            condition: always()

          # Post-E2E cleanup: remove stopped containers and dangling images
          - template: ./templates/template-docker-cleanup.yml
            parameters:
              reason: "post-e2e-csp"

          # Clean Up Database
          - bash: |
              . ./hack/e2e/run-rp-and-e2e.sh
              az cosmosdb sql database delete --name "$DATABASE_NAME" --yes --account-name "$DATABASE_ACCOUNT_NAME" --resource-group "$RESOURCEGROUP"
            displayName: Clean Up Database
            condition: always()

          # AZ CLI Logout
          - template: ./templates/template-az-cli-logout.yml
      - job: Run_E2E_Tests_For_MIWI
        timeoutInMinutes: 0
        pool:
          name: 1es-aro-ci-pool
        steps:
          # Checkout the code
          - template: ./templates/template-checkout.yml

          # Install Docker, Docker Compose, and dependencies
          - bash: |
              . ./hack/e2e/utils.sh
              install_docker_dependencies
            displayName: Install Docker and Docker Compose
          - task: Docker@2
            inputs:
              command: "login"
              containerRegistry: arointsvc #service connection name

          # Clean up stale Docker state from previous runs on this node.
          # Only removes stopped containers, dangling images, and old build cache.
          - template: ./templates/template-docker-cleanup.yml
            parameters:
              reason: "pre-e2e"

          # AZ CLI Login
          - template: ./templates/template-az-cli-login.yml
            parameters:
              azureDevOpsJSONSPN: $(aro-v4-e2e-devops-spn)

          - bash: |
              az account set -s $AZURE_SUBSCRIPTION_ID
              SECRET_SA_ACCOUNT_NAME=$(SECRET_SA_ACCOUNT_NAME) make secrets
            displayName: Fetch secrets

          - bash: |
              echo "##vso[task.setvariable variable=CI]true"
            displayName: Set CI=true

          # Override the E2E label for IndividualCI/BatchedCI (i.e. not manually
          # ran/PR jobs) to run all non-smoke tasks (default is !smoke&&!regressiontest)
          - bash: |
              echo "##vso[task.setvariable variable=E2E_LABEL]!smoke"
            displayName: Enable regression tests in CI
            condition: in(variables['Build.Reason'], 'IndividualCI', 'BatchedCI')

          # Get Kubeconfig for AKS Cluster
          - bash: |
              . secrets/env

              # Retrieve the kubeconfig
              hack/get-admin-aks-kubeconfig.sh > aks.kubeconfig

              if [ -f aks.kubeconfig ]; then
                echo "Kubeconfig retrieved successfully."
              else
                echo "Failed to retrieve Kubeconfig."
                exit 1
              fi
            displayName: Get Kubeconfig for AKS Cluster

          # Run the E2E test suite
          - bash: |
              echo "##vso[task.setvariable variable=USE_WI]true"
            displayName: Set USE_WI=true
          - bash: |
              . secrets/env
              . ./hack/e2e/run-rp-and-e2e.sh
              echo "##vso[task.setvariable variable=CLUSTER]${CLUSTER}"
              echo "##vso[task.setvariable variable=DATABASE_NAME]${DATABASE_NAME}"
            displayName: Set CLUSTER and DATABASE_NAME
          - bash: |
              # Set SKIP_MIWI_ROLE_ASSIGNMENT=true to avoid creating platform identities and assigning Azure roles during
              # MIWI pipeline runs. These operations are handled as part of cluster creation, within the e2e's BeforeSuite.
              . secrets/env
              . ./hack/e2e/run-rp-and-e2e.sh
              export SKIP_MIWI_ROLE_ASSIGNMENT=true
              . ./hack/devtools/local_dev_env.sh
              echo "Creating MIWI env file"
              create_miwi_env_file
              . ./env
              az acr login --name arosvcdev
              az acr login --name arointsvc
              deploy_e2e_db
              register_sub
              docker compose build vpn
              docker compose up -d vpn --wait
              docker compose up --quiet-pull --exit-code-from e2e e2e
              # Check if the E2E tests failed
              E2E_EXIT_CODE=$?
              if [ $E2E_EXIT_CODE -ne 0 ]; then
                echo "##vso[task.logissue type=error] E2E tests failed. Check the logs for more details."
                exit $E2E_EXIT_CODE
              else
                echo "E2E tests passed."
              fi
            displayName: ‚öôÔ∏è  Run E2E Test Suite for MIWI
          - bash: |
              . ./env
              echo "deleting mock msi service principal $MOCK_MSI_OBJECT_ID"
              az ad sp delete --id $MOCK_MSI_OBJECT_ID
            displayName: Delete Mock MSI Service Principal

          # Log the output from the services in case of failure
          - bash: |
              docker compose logs vpn
              docker compose logs selenium
              docker compose logs rp
              docker compose logs portal
              docker compose logs e2e
            displayName: Log Service Output
            condition: always()

          # Collect must-gather logs
          - bash: |
              # The script calls ./db <resource-id> which connects to the CosmosDB to fetch cluster details.
              # Since must-gather runs independently, we rebuild the binary on-demand.
              . secrets/env
              go build ./hack/db
              ./hack/get-admin-kubeconfig.sh /subscriptions/$AZURE_SUBSCRIPTION_ID/resourceGroups/$CLUSTER/providers/Microsoft.RedHatOpenShift/openShiftClusters/$CLUSTER > admin.kubeconfig
              export KUBECONFIG=$(pwd)/admin.kubeconfig
              wget -nv https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/$(OpenShiftCLIVersion)/openshift-client-linux-$(OpenShiftCLIVersion).tar.gz
              tar xf openshift-client-linux-$(OpenShiftCLIVersion).tar.gz
              ./oc adm must-gather --image arointsvc.azurecr.io/openshift4/ose-must-gather:latest
              tar cf must-gather.tar.gz must-gather.local.*
            displayName: Collect must-gather
            condition: Failed()
          # Publish the must-gather result to the pipeline
          - publish: must-gather.tar.gz
            artifact: must-gather
            displayName: Append must-gather to Pipeline
            condition: Failed()

          # Clean up Docker Compose services with diagnostics
          - bash: |
              set -x
              echo "=== Docker state before compose down ==="
              echo "--- Running containers ---"
              docker ps --format "table {{.ID}}\t{{.Image}}\t{{.Status}}\t{{.Names}}" || true
              echo "--- All containers (including stopped) ---"
              docker ps -a --format "table {{.ID}}\t{{.Image}}\t{{.Status}}\t{{.Names}}" || true
              echo "--- Docker images on this node ---"
              docker images --format "table {{.Repository}}\t{{.Tag}}\t{{.ID}}\t{{.CreatedSince}}\t{{.Size}}" || true
              echo "--- Docker disk usage ---"
              docker system df || true
              echo ""
              echo "=== Stopping compose services ==="
              docker compose down --remove-orphans || true
              rm -f aks.kubeconfig
              echo ""
              echo "=== Docker state after compose down ==="
              docker ps -a --format "table {{.ID}}\t{{.Image}}\t{{.Status}}\t{{.Names}}" || true
            displayName: Cleanup Docker Compose Services and Kubeconfig (with diagnostics)
            condition: always()

          # Post-E2E cleanup: remove stopped containers and dangling images
          - template: ./templates/template-docker-cleanup.yml
            parameters:
              reason: "post-e2e-miwi"

          # Clean Up Database
          - bash: |
              . ./hack/e2e/run-rp-and-e2e.sh
              az cosmosdb sql database delete --name "$DATABASE_NAME" --yes --account-name "$DATABASE_ACCOUNT_NAME" --resource-group "$RESOURCEGROUP"
            displayName: Clean Up Database
            condition: always()

          # AZ CLI Logout
          - template: ./templates/template-az-cli-logout.yml
